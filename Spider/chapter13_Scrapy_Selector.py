

#****** 13.1 Scrapyæ¡†æ¶çš„ä½¿ç”¨ ******



#****** 13.2 Scrapyå…¥é—¨ ******

# * è§é¡¹ç›®tutorialğŸ”¼

# å»ºScarpyé¡¹ç›®å‘½ä»¤è¡Œæµç¨‹
#   åœ¨é¡¹ç›®ç›®å½•ä¸‹
#   scrapy startproject tutorial

#   cd tutorial
#   scrapy genspider quotes quotes.toscrape.com

#   scrapy crawl quotes è¿è¡Œ


#****** 13.3 Selectorçš„ç”¨æ³• ******

# SelectoråŸºäºlxmlæ„å»ºã€‚æ”¯æŒXPathé€‰æ‹©å™¨ã€CSSé€‰æ‹©å™¨ä»¥åŠæ­£åˆ™è¡¨è¾¾å¼

# 1ã€ç›´æ¥ä½¿ç”¨

from scrapy import Selector

body = '<title>Hello World</title>'
selector = Selector(text=body)
title = selector.xpath('//title/text()').extract_first()
print(title)

# 2ã€Scrapy shell

# scrapy shell xxxx

# 3ã€XPathé€‰æ‹©å™¨

# responseæœ‰ä¸€ä¸ªå±æ€§selectorï¼Œè°ƒç”¨response.selectorè¿”å›çš„å†…å®¹ç›¸å½“äºç”¨responseçš„textæ„é€ äº†ä¸€ä¸ªSelectorå¯¹è±¡
# è€Œresponse.xpath()å’Œresponse.css()åŠŸèƒ½ä¸ä¸Šé¢çš„ç›¸åŒ

# å¯ä»¥ä¸ºextract_first()è®¾ç½®é»˜è®¤å€¼

# 4ã€CSSé€‰æ‹©å™¨

#   è·å–æ–‡æœ¬å’Œå±æ€§éœ€è¦ç”¨::textå’Œ::attr()çš„å†™æ³•

# å¯ä»¥å…ˆç”¨xpathåœ¨ç”¨CSS
#   response.xpath('//a').css('img').xpath('@src').extract()

# 5ã€æ­£åˆ™åŒ¹é…

#   response.xpath('//a/text()').re('Name:\s(.*)')
#   è¾“å‡ºæ‹¬å·å†…çš„ï¼Œåˆ—è¡¨å½¢å¼

#   response.xpath('//a/text()').re_first('Name:\s(.*)')
#   è¾“å‡ºç¬¬ä¸€ä¸ª

#   æ³¨æ„ï¼šresponseä¸èƒ½ç›´æ¥è°ƒç”¨reï¼Œå¯ä»¥å…ˆç”¨xpathå†æ­£åˆ™




#****** 13.4 Spiderçš„ç”¨æ³• ******



#****** 13.5 Downloader Middlewareçš„ç”¨æ³• ******

#   ä¿®æ”¹User-Agentã€å¤„ç†é‡å®šå‘ã€è®¾ç½®ä»£ç†ã€å¤±è´¥é‡è¯•ã€è®¾ç½®Cookiesç­‰åŠŸèƒ½éƒ½è¦å€ŸåŠ©å®ƒå®ç°

# ä½œç”¨ï¼š
#   1ã€åœ¨Schedulerè°ƒåº¦å¤„é˜Ÿåˆ—çš„Requestå‘é€ç»™DOwnloaderä¸‹è½½ä¹‹å‰ã€‚å¯¹å…¶è¿›è¡Œä¿®æ”¹
#   2ã€åœ¨ä¸‹è½½åç”Ÿæˆçš„Responseå‘é€ç»™Spiderä¹‹å‰ï¼Œå¯ä¿®æ”¹

# è‡³å°‘å®ç°ä»¥ä¸‹å…¶ä¸­ä¸€ä¸ªæ–¹æ³•ï¼Œå³å¯å®šä¹‰ä¸€ä¸ªDownloader Middleware
#   1ã€process_request(request, spider)
#       è°ƒåº¦å‰è¢«è°ƒç”¨
#   2ã€process_response(request, response, spider)
#   3ã€process_exception(request, exception, spider)







